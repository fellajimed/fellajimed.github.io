<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Geologica:wght@500;600;700&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="style.css">
    <title>Publications - Mohammed Fellaji</title>
    <link rel="icon" href="dev_pdp.png">
  </head>

  <body>
    <header>
      <div class="container-header">
        <div class="home-div"> <a href="index.html"> Mohammed Fellaji </a> </div>

        <div class="menu-nav">
          <nav class="navigation">
            <ul class="option">
              <input type="checkbox" id="collapse" hidden>
              <label class="menu-button" for="collapse"> </label>
              <li class="menu-item"> <a class="section-item" href="index.html"> About </a> </li>
              <li class="menu-item"> <a class="section-item" href="https://fellajimed.github.io/files/Resume_Mohammed_Fellaji.pdf" target="_blank"> Resume </a> </li>
              <li class="menu-item"> <a class="section-item" href="publications.html"> Publications </a> </li>
              <!-- <li class="menu-item"> <a class="section-item" href="/blog"> Blog </a> </li> -->
              <li class="menu-item"> <a class="section-item" href="mailto:&#102;&#101;&#108;&#108;&#97;&#106;&#105;&#109;&#111;&#104;&#97;&#109;&#109;&#101;&#100;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;"> Contact </a> </li>
            </ul>
          </nav>
        </div>
      </div>
    </header>

    <main class="content">
      <div class="container" id="publications">
        <h1>My Publications</h1>

        <div class="publication-list">

          <div class="publication-item">
            <div class="pub-details">
              <h2>Epistemic Calibration for Bayesian Deep Learning: Principles, Issues and Solutions</h2>
              <p class="pub-authors">Mohammed Fellaji</p>
              <p class="pub-venue">PhD Thesis, 2025</p>
            </div>

            <input type="checkbox" id="abstract-thesis" class="abstract-checkbox" hidden>

            <div class="pub-button-row">
              <label for="abstract-thesis" class="abstract-button"></label>
              <a href="https://fellajimed.github.io/files/thesis_Mohammed_Fellaji.pdf" class="download-button" target="_blank">Download PDF</a>
            </div>

            <div class="abstract-content">
              <p>Although most deep learning models provide probabilistic distributions as a predictive output, their evaluation often relies mainly on raw performance metrics (e.g. accuracy for classification) insensitive to the uncertainty expressed by these distributions.Yet, the inherent restrictions on the generalization ability of these models make them extremely unlikely to reach flawless performance on new data, hence advocating for the importance of examining the confidence of the predictions. In this regard, the field of model calibration has recently gained considerable attention in the deep learning community, with the aim of encouraging reliable predictions. Meanwhile, the development of models like Bayesian neural networks, deep ensemble or evidential deep models has made it possible to estimate the level of epistemic uncertainty, inherent to the learning process, in complement to the aleatoric uncertainty already estimated by standard models.

              <br>
              <br>

              While the quality of predictive/aleatoric uncertainty can be measured by well-established calibration methods, the same cannot be said about epistemic uncertainty. Since the latter is considered the ideal score in a range of applications, it is therefore of utmost importance to explore its calibration properties, which has rarely been addressed in the literature. When attempting to define epistemic calibration, more challenges arise on how to formalize this calibration, assuming its existence. For instance, it may be worth considering whether it is feasible to study it similarly to model calibration, or at the very least, based on fundamental principles. Throughout this thesis, we have attempted to overcome these challenges by conducting work of both a theoretical and experimental nature in the specific context of deep classifiers.

              <br>
              <br>

              After reviewing the state of the art to quantifying probabilistic uncertainty, especially in the field of deep models, and given the difficulty of quantitatively calibrating epistemic uncertainty, we first define formally two elementary principles that epistemic uncertainty should ideally satisfy to our view: data-related and model-related principles. Indeed, as epistemic uncertainty is associated with knowledge in the model, it should decrease with the amount of available data and increase with the expressivity/complexity of the model. Empirically, and on a variety of datasets, we show that commonly used Bayesian models or alternatives do not fully verify these fundamental principles. Therefore, we argue that these models lack epistemic calibration, and we refer to this phenomenon as the epistemic uncertainty hole.

              <br>
              <br>

              Considering the critical role that the prior plays in shaping epistemic uncertainty, we investigate how much this failure of the tested models is due to an inadequate choice of prior. To this end, we introduce Conflictual loss, a loss function that favors diversity of the outputs thanks to the use of an uninformative prior. We then experimentally show that Conflictual loss leads to a better calibrated epistemic uncertainty and does not suffer from the epistemic uncertainty hole. Additionally, special inputs were investigated, which were either noisy samples or drawn from the test set, to understand the evolution of different sources of uncertainties. Furthermore, we analyze the specificities of the conflictual diversity in the parameters space, and highlight the differences with deep ensembles. Building on the findings of this analysis, a compact version of the model was formalized, further emphasizing the benefits of the uninformative prior. Finally, the models were evaluated on popular applications such as out-of-distribution (OOD) detection and Bayesian active learning.</p>
            </div>
          </div>

          <br>

          <div class="publication-item">
            <div class="pub-details">
              <h2>On the Calibration of Epistemic Uncertainty: Principles, Paradoxes and Conflictual Loss</h2>
              <p class="pub-authors">Mohammed Fellaji, Frédéric Pennerath, Brieuc Conan-Guez, Miguel Couceiro</p>
              <p class="pub-venue">ECML PKDD, 2024</p>
            </div>

            <input type="checkbox" id="abstract-ecml2024" class="abstract-checkbox" hidden>

            <div class="pub-button-row">
              <label for="abstract-ecml2024" class="abstract-button"></label>
              <a href="https://arxiv.org/pdf/2407.12211" class="download-button" target="_blank">Download PDF</a>
            </div>

            <div class="abstract-content">
              <p>The calibration of predictive distributions has been widely studied in deep learning, but the same cannot be said about the more specific epistemic uncertainty as produced by Deep Ensembles, Bayesian Deep Networks, or Evidential Deep Networks. Although measurable, this form of uncertainty is difficult to calibrate on an objective basis as it depends on the prior for which a variety of choices exist. Nevertheless, epistemic uncertainty must in all cases satisfy two formal requirements: first, it must decrease when the training dataset gets larger and, second, it must increase when the model expressiveness grows. Despite these expectations, our experimental study shows that on several reference datasets and models, measures of epistemic uncertainty violate these requirements, sometimes presenting trends completely opposite to those expected. These paradoxes between expectation and reality raise the question of the true utility of epistemic uncertainty as estimated by these models. A formal argument suggests that this disagreement is due to a poor approximation of the posterior distribution rather than to a flaw in the measure itself. Based on this observation, we propose a regularization function for deep ensembles, called conflictual loss in line with the above requirements. We emphasize its strengths by showing experimentally that it restores both requirements of epistemic uncertainty, without sacrificing either the performance or the calibration of the deep ensembles.</p>
            </div>
          </div>

          <br>

          <div class="publication-item">
            <div class="pub-details">
              <h2>The Epistemic Uncertainty Hole: an issue of Bayesian Neural Networks</h2>
              <p class="pub-authors">Mohammed Fellaji, Frédéric Pennerath</p>
              <p class="pub-venue">CAp, 2023</p>
            </div>

            <input type="checkbox" id="abstract-cap2023" class="abstract-checkbox" hidden>

            <div class="pub-button-row">
              <label for="abstract-cap2023" class="abstract-button"></label>
              <a href="https://arxiv.org/pdf/2407.01985" class="download-button" target="_blank">Download PDF</a>
            </div>

            <div class="abstract-content">
              <p>Bayesian Deep Learning (BDL) gives access not only to aleatoric uncertainty, as standard neural networks already do, but also to epistemic uncertainty, a measure of confidence a model has in its own predictions. In this article, we show through experiments that the evolution of epistemic uncertainty metrics regarding the model size and the size of the training set, goes against theoretical expectations. More precisely, we observe that the epistemic uncertainty collapses literally in the presence of large models and sometimes also of little training data, while we expect the exact opposite behaviour. This phenomenon, which we call "epistemic uncertainty hole", is all the more problematic as it undermines the entire applicative potential of BDL, which is based precisely on the use of epistemic uncertainty. As an example, we evaluate the practical consequences of this uncertainty hole on one of the main applications of BDL, namely the detection of out-of-distribution samples.</p>
            </div>
          </div>

        </div>
      </div>
    </main>

  </body>
</html>